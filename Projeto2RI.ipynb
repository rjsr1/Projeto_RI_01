{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 Recuperacao da Informacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parte referente aos métodos de processamento de consulta, considerando o modelo de espaço\n",
    "#de vetores e método de comparação de ranqueamentos\n",
    "\n",
    "#Autor: Marcus Vinicius Silva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals\n",
    "\n",
    " \n",
    "from time import time\n",
    "import timeit\n",
    "import codecs\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#essa funcao recebe o nome do arquivo html, e adiciona a representacao em texto dele na lista de strings\n",
    "\n",
    "def parse_html(file_name, lista):\n",
    "    \n",
    "    f = codecs.open(file_name, 'r', 'utf-8', 'replace') #pode dar problema usar esse encoding, mas\n",
    "    #a opcao replace ignora erros de caracters substituindo por ? na string, que no final das contas\n",
    "    #vai ser desconsiderado do vocabulario\n",
    "    \n",
    "    \n",
    "    \n",
    "    with f as x_file:\n",
    "        soup = BeautifulSoup(x_file.read())\n",
    "    \n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        \n",
    "        script.decompose()\n",
    "        \n",
    "    # get text\n",
    "    text = ' '.join(  soup.stripped_strings  )\n",
    "\n",
    "    lista.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#essa funcao le todos os arquivos html para montar a base de dados\n",
    "\n",
    "def ler_dados(lista):    \n",
    "    \n",
    "    \n",
    "    PATH_FILE = './paginas_positivas/trainset' #lendo da pasta de paginas positivas\n",
    "    i = 1\n",
    "    limite = NUM_DOCS\n",
    "    \n",
    "    #ler paginas positivas\n",
    "    while i < (limite + 1):\n",
    "        file_name = PATH_FILE + ' (' +  str(i) + ').htm'  \n",
    "        parse_html(file_name, lista)\n",
    "        i += 1    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo espaço de vetores sem tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarcusVinicius\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\MarcusVinicius\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#lista dos documentos\n",
    "NUM_DOCS = 50\n",
    "lista_dados = [] #tam definido na funcao ler_dados\n",
    "fator_norm = np.zeros(NUM_DOCS) #fator usado para normalizacao no cosine score\n",
    "indice = []\n",
    "\n",
    "#ler arquivos html e gerar lista de docs\n",
    "ler_dados(lista_dados)\n",
    "#calcular freq dos termos, representacao bag of words\n",
    "vectorizer = CountVectorizer(strip_accents='unicode', max_df=0.9)\n",
    "base_docs = vectorizer.fit_transform(lista_dados)\n",
    "\n",
    "\n",
    "for i in range(NUM_DOCS):\n",
    "    fator_norm[i] = len(base_docs[i].indices)\n",
    "    \n",
    "\n",
    "#Construcao do indice\n",
    "\n",
    "#montando a cabeca dos postings\n",
    "for chave in vectorizer.vocabulary_.keys():\n",
    "    indice.append([chave,0, []]) #o segundo valor na verdade eh o doc_freq\n",
    "\n",
    "#criando os postings\n",
    "for post_head in indice:\n",
    "    for i in range(np.shape(base_docs)[0]):\n",
    "        coluna = vectorizer.vocabulary_[post_head[0]] #procura index do termo na base de dados\n",
    "        #representada em bag of words\n",
    "        if base_docs[i,coluna] != 0:\n",
    "            post_head[2].append([i, base_docs[i,coluna]]) #i eh o id do doc e basedocs armazena a freq\n",
    "            \n",
    "    post_head[1] = NUM_DOCS/len(post_head[2]) #frequencia dos documentos que contem o termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rio',\n",
       "  1.6666666666666667,\n",
       "  [[0, 3],\n",
       "   [1, 1],\n",
       "   [2, 1],\n",
       "   [3, 1],\n",
       "   [4, 1],\n",
       "   [5, 1],\n",
       "   [6, 1],\n",
       "   [7, 3],\n",
       "   [9, 1],\n",
       "   [10, 1],\n",
       "   [12, 1],\n",
       "   [13, 1],\n",
       "   [14, 3],\n",
       "   [16, 1],\n",
       "   [17, 8],\n",
       "   [18, 1],\n",
       "   [20, 3],\n",
       "   [21, 1],\n",
       "   [22, 1],\n",
       "   [23, 6],\n",
       "   [24, 1],\n",
       "   [25, 1],\n",
       "   [26, 8],\n",
       "   [35, 3],\n",
       "   [43, 1],\n",
       "   [44, 3],\n",
       "   [45, 1],\n",
       "   [46, 3],\n",
       "   [47, 3],\n",
       "   [48, 8]]],\n",
       " ['8v',\n",
       "  4.545454545454546,\n",
       "  [[0, 2],\n",
       "   [7, 3],\n",
       "   [13, 1],\n",
       "   [14, 3],\n",
       "   [15, 1],\n",
       "   [17, 3],\n",
       "   [18, 3],\n",
       "   [20, 3],\n",
       "   [26, 4],\n",
       "   [32, 2],\n",
       "   [48, 3]]],\n",
       " ['flex',\n",
       "  1.9230769230769231,\n",
       "  [[0, 2],\n",
       "   [1, 2],\n",
       "   [2, 2],\n",
       "   [4, 2],\n",
       "   [7, 3],\n",
       "   [8, 3],\n",
       "   [11, 4],\n",
       "   [12, 2],\n",
       "   [13, 4],\n",
       "   [14, 3],\n",
       "   [15, 5],\n",
       "   [16, 2],\n",
       "   [17, 1],\n",
       "   [20, 3],\n",
       "   [23, 1],\n",
       "   [24, 1],\n",
       "   [25, 1],\n",
       "   [26, 6],\n",
       "   [27, 1],\n",
       "   [32, 1],\n",
       "   [33, 2],\n",
       "   [35, 3],\n",
       "   [44, 3],\n",
       "   [47, 3],\n",
       "   [48, 1],\n",
       "   [49, 1]]],\n",
       " ['4p',\n",
       "  2.9411764705882355,\n",
       "  [[0, 2],\n",
       "   [1, 2],\n",
       "   [2, 2],\n",
       "   [3, 2],\n",
       "   [4, 2],\n",
       "   [5, 2],\n",
       "   [6, 2],\n",
       "   [7, 3],\n",
       "   [8, 1],\n",
       "   [13, 1],\n",
       "   [14, 3],\n",
       "   [20, 3],\n",
       "   [27, 1],\n",
       "   [35, 3],\n",
       "   [44, 3],\n",
       "   [46, 3],\n",
       "   [47, 3]]],\n",
       " ['manual',\n",
       "  1.4285714285714286,\n",
       "  [[0, 4],\n",
       "   [1, 4],\n",
       "   [2, 4],\n",
       "   [3, 4],\n",
       "   [4, 4],\n",
       "   [7, 4],\n",
       "   [8, 2],\n",
       "   [9, 1],\n",
       "   [10, 1],\n",
       "   [11, 2],\n",
       "   [13, 2],\n",
       "   [14, 4],\n",
       "   [15, 2],\n",
       "   [18, 1],\n",
       "   [19, 3],\n",
       "   [20, 4],\n",
       "   [21, 1],\n",
       "   [22, 1],\n",
       "   [24, 1],\n",
       "   [25, 1],\n",
       "   [28, 1],\n",
       "   [30, 1],\n",
       "   [32, 1],\n",
       "   [34, 1],\n",
       "   [36, 2],\n",
       "   [37, 1],\n",
       "   [38, 2],\n",
       "   [39, 1],\n",
       "   [40, 2],\n",
       "   [41, 2],\n",
       "   [42, 1],\n",
       "   [43, 1],\n",
       "   [45, 1],\n",
       "   [47, 4],\n",
       "   [49, 2]]]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FUNCAO PARA CALCULAR RESULTADO RANQUEADO DA CONSULTA\n",
    "\n",
    "#assume q como uma string split da consulta em texto livre\n",
    "def cosine_sim(q, k):\n",
    "    scores = np.zeros(NUM_DOCS)\n",
    "    #vetor relativo a norma dos docs eh global\n",
    "    for term in q:\n",
    "        post_list_t = [item for item in indice if item[0] == term]\n",
    "        \n",
    "        if post_list_t: #pode ocorrer de o termo da pesquisa nao estar indexado\n",
    "            post_list_t = post_list_t[0]\n",
    "            for tupla in post_list_t[2]: #tupla 0 eh o id e tupla 1 do peso do termo no doc\n",
    "                scores[tupla[0]] += post_list_t[1]*tupla[1]    #peso do termo na consulta vezes no doc \n",
    "        \n",
    "    \n",
    "    ind_scores = []\n",
    "    \n",
    "    for i in range(NUM_DOCS):\n",
    "        scores[i] = scores[i] / (fator_norm[i]*len(q)) #consulta tambem deve ser normalizada\n",
    "        ind_scores.append([scores[i], i])\n",
    "    \n",
    "    ind_scores.sort()\n",
    "    \n",
    "        \n",
    "    \n",
    "    return ind_scores[:-(k + 1):-1] #retornar k melhores resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.015644813332674604, 38], [0.013588263588263589, 17]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consulta = 'ford ranger 2007'\n",
    "resultado_consulta = cosine_sim(consulta.split(), 2)\n",
    "resultado_consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo espaço de vetores com tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "base_docs2 = tfidf_transformer.fit_transform(base_docs)\n",
    "\n",
    "#cria indice separado\n",
    "indice2 = []\n",
    "\n",
    "#montando a cabeca dos postings\n",
    "for chave in vectorizer.vocabulary_.keys():\n",
    "    indice2.append([chave,0, []]) #o segundo valor na verdade eh o doc_freq\n",
    "\n",
    "#criando os postings\n",
    "for post_head in indice:\n",
    "    for i in range(np.shape(base_docs)[0]):\n",
    "        coluna = vectorizer.vocabulary_[post_head[0]] #procura index do termo na base de dados\n",
    "        #representada em bag of words\n",
    "        if base_docs[i,coluna] != 0:\n",
    "            post_head[2].append([i, base_docs2[i,coluna]]) #i eh o id do doc e basedocs armazena a freq\n",
    "            \n",
    "    post_head[1] = NUM_DOCS/len(post_head[2]) #frequencia dos documentos que contem o termo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcao para calcular indice kendal tau\n",
    "#assume receber dois resultados de consultas no formato [[peso, id]]\n",
    "#assume resultados contendo os mesmos ids, possivelmente com rankings diferentes\n",
    "def kendal_tau(res1, res2):\n",
    "    \n",
    "    lista_ids1 = [i[1] for i in res1]\n",
    "    lista_ids2 = [i[1] for i in res2]\n",
    "    \n",
    "    \n",
    "    pares1 = list(it.permutations(lista_ids1, 2))\n",
    "    pares2 = list(it.permutations(lista_ids2, 2))\n",
    "    pares_ajust1 = [] #vai eliminar alguns pares ordenados da permutacao original\n",
    "    pares_ajust2 = []\n",
    "    \n",
    "    #dicionarios para armazenar valor do indice original do doc na sua resposta, acessando por id\n",
    "    dic_indx1 = {}\n",
    "    dic_indx2 = {}\n",
    "    \n",
    "    k = len(res1)\n",
    "    \n",
    "    for i, elem in enumerate(res1, start = 0):\n",
    "        dic_indx1[elem[1]] = i #salva a posicao do doc no resultado 1, para acessar por id depois\n",
    "    \n",
    "    for j, elem in enumerate(res2, start = 0):\n",
    "        dic_indx2[elem[1]] = j #salva a posicao do doc no resultado 1, para acessar por id depois\n",
    "        \n",
    "    \n",
    "    for elem in pares1:\n",
    "        if lista_ids1.index(elem[0]) < lista_ids1.index(elem[1]):\n",
    "            pares_ajust1.append([elem[0], elem[1]])\n",
    "            \n",
    "    for elem in pares2:\n",
    "        if lista_ids2.index(elem[0]) < lista_ids2.index(elem[1]):\n",
    "            pares_ajust2.append([elem[0], elem[1]])    \n",
    "    \n",
    "    num_discor = 0 #numero de pares de ids discordantes entre os rankings\n",
    "    #procurar pares discordantes\n",
    "    for elem in pares_ajust1:\n",
    "        if (dic_indx1[elem[0]] < dic_indx1[elem[1]]) and (dic_indx2[elem[0]] > dic_indx2[elem[1]]):\n",
    "            num_discor += 1\n",
    "        if (dic_indx1[elem[0]] > dic_indx1[elem[1]]) and (dic_indx2[elem[0]] < dic_indx2[elem[1]]):\n",
    "            num_discor += 1\n",
    " \n",
    "    for elem in pares_ajust2:\n",
    "        if (dic_indx2[elem[0]] < dic_indx2[elem[1]]) and (dic_indx1[elem[0]] > dic_indx1[elem[1]]):\n",
    "            num_discor += 1\n",
    "        if (dic_indx2[elem[0]] > dic_indx2[elem[1]]) and (dic_indx1[elem[0]] < dic_indx1[elem[1]]):\n",
    "            num_discor += 1\n",
    "    \n",
    "    \n",
    "    return 1 - 2*num_discor/(k*(k-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = [[50,123], [40,84], [30,56], [20,6] , [10,8]]\n",
    "res2 = [[50,56], [40,123], [30,84], [20,8] , [10,6]]\n",
    "\n",
    "\n",
    "teste_n = kendal_tau(res1, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
